{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30a7727c-9864-49cd-af55-052785390511",
   "metadata": {},
   "source": [
    "# Part 3: \n",
    "\n",
    "Students:\n",
    "- Konstantinos Nikoletos \n",
    "- Konstantinos Plas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7aaf8d-bce3-4af2-86cb-b533826d0bad",
   "metadata": {},
   "source": [
    "## Question 3.1: De-Duplication with Locality Sensitive Hashing\n",
    "\n",
    "### Description\n",
    "In this question you will be given a train set file with small texts. Every text is a Quora\n",
    "question. You will also be given a test set in the same format. The purpose of this question\n",
    "is to find how many of the documents in the test-set already exist in the train-set. As\n",
    "duplicate we define document pairs with similarity more than a threshold τ=0.8. You have to\n",
    "search for duplicate documents using the appropriate LSH family in order to reduce the time\n",
    "required for the detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f667cf25-4fa3-4d90-86b0-ba27dbc16c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b693d17-055e-43ad-98ef-9c0c2784d061",
   "metadata": {},
   "source": [
    "## Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70952213-80de-4be6-a351-4cc8e924e5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 1)\n",
      "(1000, 1)\n",
      "Train data shape:  (3000, 1)\n",
      "                                            Question\n",
      "0  What is the step by step guide to invest in sh...\n",
      "1  What is the story of Kohinoor (Koh-i-Noor) Dia...\n",
      "2  How can I increase the speed of my internet co...\n",
      "3  Why am I mentally very lonely? How can I solve...\n",
      "4  Which one dissolve in water quikly sugar, salt...\n",
      "\n",
      "Test data shape:  (1000, 1)\n",
      "                                            Question\n",
      "0  What can someone do if they've lost the wirele...\n",
      "1            Why India need to elect Prime minister?\n",
      "2     How can I make money online with free of cost?\n",
      "3  Does MDMA affect the first and higher order mo...\n",
      "4  I am a Saudi National and have \"SR 3 million\" ...\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('./Part-3/data/train_q_3.1.csv', names=['Question'],  delimiter='\\t', header=0)\n",
    "test_data = pd.read_csv('./Part-3/data/test_without_labels_q_3.1.csv',  delimiter='\\t')\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "# train_data = train_data.head(100)\n",
    "# test_data = test_data.head(100)\n",
    "\n",
    "print(\"Train data shape: \", train_data.shape)\n",
    "print(train_data.head())\n",
    "\n",
    "print(\"\\nTest data shape: \", test_data.shape)\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "242c290b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>What type of diets can you follow to lose 5 po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>Which is the best commerce college in Mangalore?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>Is networking a good field to have a career in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>Can I use letter stamps as postcard stamps?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>How much water should a normal, healthy adult ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Question\n",
       "0     What is the step by step guide to invest in sh...\n",
       "1     What is the story of Kohinoor (Koh-i-Noor) Dia...\n",
       "2     How can I increase the speed of my internet co...\n",
       "3     Why am I mentally very lonely? How can I solve...\n",
       "4     Which one dissolve in water quikly sugar, salt...\n",
       "...                                                 ...\n",
       "2995  What type of diets can you follow to lose 5 po...\n",
       "2996   Which is the best commerce college in Mangalore?\n",
       "2997  Is networking a good field to have a career in...\n",
       "2998        Can I use letter stamps as postcard stamps?\n",
       "2999  How much water should a normal, healthy adult ...\n",
       "\n",
       "[3000 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae660040-3839-4cd3-a7c6-dbf17b364ffd",
   "metadata": {},
   "source": [
    "## Pre-processing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81d890e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "stop_words_pypi = set(get_stop_words('en'))\n",
    "# print(stop_words_pypi)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "# print(stop_words_nltk)\n",
    "\n",
    "manual_stop_words = {'include', 'way', 'work', 'look', 'add', 'time', 'year', 'one', \\\n",
    "                     'month', 'day', 'help', 'think', 'tell', 'new', 'said', 'say',\\\n",
    "                     'need', 'come', 'good', 'set', 'want', 'people', 'use', 'day', 'week', 'know'}\n",
    "\n",
    "stop_words= stop_words_nltk.union(stop_words_pypi)\n",
    "stop_words = stop_words.union(manual_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09f5951c-55e3-4bdb-9987-627628626ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    processed_text = text.lower()\n",
    "    processed_text = re.sub(r'\\W', ' ', str(text))\n",
    "    processed_text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_text)\n",
    "    processed_text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_text)\n",
    "    processed_text = re.sub(r'\\s+', ' ', processed_text, flags=re.I)\n",
    "    processed_text = re.sub(r'^b\\s+', '', processed_text)\n",
    "\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in processed_text.split() if word not in stop_words]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    processed_text = ' '.join(tokens)\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8af9046b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Reading from file\n",
      "[TEST] Reading from file\n"
     ]
    }
   ],
   "source": [
    "# from tqdm.notebook import tqdm\n",
    "import os\n",
    "from tqdm.auto import tqdm  # for notebooks\n",
    "tqdm.pandas()\n",
    "\n",
    "preprocessed_file_path_train = 'pre_train_31.csv'\n",
    "if not os.path.exists(preprocessed_file_path_train):\n",
    "    print(\"[TRAIN] Preprocessing text...\")\n",
    "    train_data['text'] = train_data['Question'].progress_apply(preprocess_text)\n",
    "    print(\"[TRAIN] Preprocessing text done.\")\n",
    "    train_data.to_csv(preprocessed_file_path_train, index=False)\n",
    "else:\n",
    "    print(\"[TRAIN] Reading from file\")\n",
    "    train_data = pd.read_csv(preprocessed_file_path_train)\n",
    "\n",
    "preprocessed_file_path_test = 'pre_test_31.csv'\n",
    "if not os.path.exists(preprocessed_file_path_test):\n",
    "    print(\"[TEST] Preprocessing text...\")\n",
    "    test_data['text'] = test_data['Question'].progress_apply(preprocess_text)\n",
    "    print(\"[TEST] Preprocessing text done.\")\n",
    "    test_data.to_csv(preprocessed_file_path_test, index=False)\n",
    "else:\n",
    "    print(\"[TEST] Reading from file\")\n",
    "    test_data = pd.read_csv(preprocessed_file_path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eafdeed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the story of Kohinoor (Koh-i-Noor) Diamond?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['Question'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1420df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What story Kohinoor Koh Noor Diamond'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['text'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962b0e29-7661-418f-9758-3c335fea0689",
   "metadata": {},
   "source": [
    "## Data vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26b16adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from datasketch import MinHashLSH, MinHash\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90f48a67-b32c-4a86-b5dc-5b35e246ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(train_data['text'])\n",
    "X_test_tfidf = vectorizer.transform(test_data['text'])\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_count = vectorizer.fit_transform(train_data['text'])\n",
    "X_test_count = vectorizer.transform(test_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaa55413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert sparse matrices to dense arrays\n",
    "# X_train_dense = X_train_tfidf.toarray()\n",
    "# X_test_dense = X_test_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71b8050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35af8af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d19b14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7505c983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "954901c6",
   "metadata": {},
   "source": [
    "## Cosine Similarity: Random projection LSH family."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b72e48",
   "metadata": {},
   "source": [
    "## Exact Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bbdb315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1813c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_count\n",
    "X_test = X_test_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4742cf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates: 99\n",
      "Query time: 0.04524850845336914\n"
     ]
    }
   ],
   "source": [
    "num_duplicates = 0\n",
    "\n",
    "total_time = time.time()\n",
    "\n",
    "Y = cosine_similarity(X_test_count, X_train_count)\n",
    "num_duplicates += len(np.where((Y > 0.8).any(axis=1))[0])\n",
    "# print(np.where((Y > 0.8).any(axis=1)))\n",
    "print('Duplicates: {}'.format(num_duplicates))\n",
    "print('Query time: {}'.format(time.time() - total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8c0034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import jaccard, cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0b0c8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates: 99\n"
     ]
    }
   ],
   "source": [
    "from sklearn import random_projection\n",
    "\n",
    "transformer = random_projection.GaussianRandomProjection(n_components=10)\n",
    "\n",
    "X_new = transformer.fit_transform(X_train)\n",
    "X_new = X_new > 0\n",
    "better_hash = []\n",
    "for c in X_new:\n",
    "  better_hash.append(sum([j*(2**i) for i,j in list(enumerate(reversed(c)))]))\n",
    "better_hash = np.array(better_hash)\n",
    "\n",
    "train_data['hash'] = better_hash\n",
    "hash_value = train_data['hash'].unique()\n",
    "\n",
    "# print(hash_value)\n",
    "\n",
    "num_duplicates = 0\n",
    "\n",
    "true_duplicates = np.zeros((X_test.shape[0], X_train.shape[0]))\n",
    "\n",
    "for hash_value in train_data['hash'].unique():\n",
    "#     print(hash_value)\n",
    "    xx = train_data[train_data['hash']==hash_value].index\n",
    "#     print(xx)\n",
    "    bucket = X_train[xx]\n",
    "    for i in xx:\n",
    "#         print( X_train[i])\n",
    "        sim = cosine_similarity(X_test, X_train[i])\n",
    "        z = np.where((sim > 0.8).any(axis=1))[0]\n",
    "        for zz in z:\n",
    "            true_duplicates[zz, i] = 1\n",
    "        \n",
    "#     print(Y.shape)\n",
    "#     z = np.where((Y > 0.8).any(axis=1))[0]\n",
    "#     print(z)\n",
    "#     true_duplicates[]\n",
    "#     true_d\n",
    "#     num_duplicates += len(np.where((Y > 0.8).any(axis=1))[0])\n",
    "#     true_knn =  NearestNeighbors(n_neighbors=2, algorithm='brute', metric='cosine').fit(bucket)\n",
    "#     true_knn_distances, true_knn_indices = true_knn.kneighbors(X_test)\n",
    "    \n",
    "#     if an\n",
    "    \n",
    "#     print(true_knn_distances)\n",
    "print('Duplicates: {}'.format(len(np.where((true_duplicates > 0).any(axis=1))[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866998a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e48fda5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True, False, ...,  True,  True,  True],\n",
       "       [False, False, False, ...,  True,  True,  True],\n",
       "       [ True,  True, False, ..., False,  True,  True],\n",
       "       ...,\n",
       "       [False,  True, False, ..., False, False,  True],\n",
       "       [False,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ..., False,  True, False]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7bd230e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LSHForest' from 'sklearn.neighbors' (/home/nikoletos-ubuntu/.local/lib/python3.8/site-packages/sklearn/neighbors/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_497/1604938461.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSHForest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# # Function to calculate cosine similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# def cosine_similarity(a, b):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LSHForest' from 'sklearn.neighbors' (/home/nikoletos-ubuntu/.local/lib/python3.8/site-packages/sklearn/neighbors/__init__.py)"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import LSHForest\n",
    "\n",
    "# # Function to calculate cosine similarity\n",
    "# def cosine_similarity(a, b):\n",
    "#     return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Function to perform LSH with Random Projection\n",
    "def lsh_cosine_similarity(train_features, test_features, threshold, k):\n",
    "    # Build LSH model\n",
    "    lshf = LSHForest(n_estimators=10, n_candidates=50, random_state=42, n_neighbors=5)\n",
    "    build_start_time = time.time()\n",
    "    lshf.fit(train_features)\n",
    "    build_time = time.time() - build_start_time\n",
    "\n",
    "    # Query LSH model\n",
    "    query_start_time = time.time()\n",
    "    distances, indices = lshf.kneighbors(test_features)\n",
    "    query_time = time.time() - query_start_time\n",
    "\n",
    "    # Calculate total time\n",
    "    total_time = build_time + query_time\n",
    "\n",
    "    # Count duplicates based on threshold\n",
    "    num_duplicates = sum([1 for distance in distances if distance[0] > threshold])\n",
    "\n",
    "    return build_time, query_time, total_time, num_duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8f2bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loop through different values of K (1 to 10)\n",
    "for k in range(1, 11):\n",
    "    # Apply Gaussian Random Projection with n_components=10\n",
    "    transformer = random_projection.GaussianRandomProjection(n_components=10, random_state=42)\n",
    "    X_train_transformed = transformer.fit_transform(X_train)\n",
    "    X_test_transformed = transformer.transform(X_test)\n",
    "\n",
    "    # Set the threshold for similarity\n",
    "    similarity_threshold = 0.8\n",
    "\n",
    "    # Perform LSH with Cosine Similarity\n",
    "    build_time, query_time, total_time, num_duplicates = lsh_cosine_similarity(X_train_transformed, X_test_transformed, similarity_threshold, k)\n",
    "\n",
    "    # Print results in a table\n",
    "    print(f\"LSH-Cosine\\t{build_time:.3f}\\t{query_time:.3f}\\t{total_time:.3f}\\t{num_duplicates}\\tK={k}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
