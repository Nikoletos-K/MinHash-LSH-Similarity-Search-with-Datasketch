{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30a7727c-9864-49cd-af55-052785390511",
   "metadata": {},
   "source": [
    "# Part 2: Nearest Neighbor Search with Locality Sensitive Hashing\n",
    "\n",
    "\n",
    "\n",
    "Students:\n",
    "- Konstantinos Nikoletos \n",
    "- Konstantinos Plas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7aaf8d-bce3-4af2-86cb-b533826d0bad",
   "metadata": {},
   "source": [
    "## Question 2.1: Nearest Neighbor Search without and with Locality Sensitive Hashing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f667cf25-4fa3-4d90-86b0-ba27dbc16c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/nikoletos-\n",
      "[nltk_data]     ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/nikoletos-\n",
      "[nltk_data]     ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b693d17-055e-43ad-98ef-9c0c2784d061",
   "metadata": {},
   "source": [
    "## Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70952213-80de-4be6-a351-4cc8e924e5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (111795, 4)\n",
      "       Id                                              Title  \\\n",
      "0  227464  Netflix is coming to cable boxes, and Amazon i...   \n",
      "1  244074  Pharrell, Iranian President React to Tehran 'H...   \n",
      "2   60707                    Wildlife service seeks comments   \n",
      "3   27883  Facebook teams up with Storyful to launch 'FB ...   \n",
      "4  169596           Caesars plans US$880 mln New York casino   \n",
      "\n",
      "                                             Content          Label  \n",
      "0   if you subscribe to one of three rinky-dink (...  Entertainment  \n",
      "1   pharrell, iranian president react to tehran '...  Entertainment  \n",
      "2   the u.s. fish and wildlife service has reopen...     Technology  \n",
      "3   the very nature of social media means it is o...     Technology  \n",
      "4   caesars plans us$880 mln new york casino jul ...       Business  \n",
      "\n",
      "Test data shape:  (47912, 3)\n",
      "       Id                                              Title  \\\n",
      "0  262120  Tracy Morgan upgraded to fair condition after ...   \n",
      "1  175132  Smartphones Weigh on Samsung Electronics as Gu...   \n",
      "2  218739  FBI denies fumbling testimony on 'X-Men' direc...   \n",
      "3  253483  Bachelorette 2014 Spoilers: Week 3 Recap ??? E...   \n",
      "4  224109  Barack Obama honours Frankie Knuckles in lette...   \n",
      "\n",
      "                                             Content  \n",
      "0   actor and comedian tracy morgan has been upgr...  \n",
      "1  samsung electronics co ltd on tuesday issued u...  \n",
      "2   michael f. egan iii said in a press conferenc...  \n",
      "3   i am having mixed emotions for what is about ...  \n",
      "4   u.s. president barack obama has paid a specia...  \n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('./Part-1/data/train.csv', sep=',')\n",
    "test_data = pd.read_csv('./Part-1/data/test_without_labels.csv', sep=',')\n",
    "\n",
    "print(\"Train data shape: \", train_data.shape)\n",
    "print(train_data.head())\n",
    "train_data['text'] = train_data['Title'] + \" \" + train_data['Content']\n",
    "\n",
    "print(\"\\nTest data shape: \", test_data.shape)\n",
    "print(test_data.head())\n",
    "test_data['text'] = test_data['Title'] + \" \" + test_data['Content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae660040-3839-4cd3-a7c6-dbf17b364ffd",
   "metadata": {},
   "source": [
    "## Pre-processing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "187d654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "stop_words_pypi = set(get_stop_words('en'))\n",
    "# print(stop_words_pypi)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "# print(stop_words_nltk)\n",
    "\n",
    "manual_stop_words = {'include', 'way', 'work', 'look', 'add', 'time', 'year', 'one', \\\n",
    "                     'month', 'day', 'help', 'think', 'tell', 'new', 'said', 'say',\\\n",
    "                     'need', 'come', 'good', 'set', 'want', 'people', 'use', 'day', 'week', 'know'}\n",
    "\n",
    "stop_words= stop_words_nltk.union(stop_words_pypi)\n",
    "stop_words = stop_words.union(manual_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e968a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    processed_text = text.lower()\n",
    "    processed_text = re.sub(r'\\W', ' ', str(text))\n",
    "    processed_text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_text)\n",
    "    processed_text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_text)\n",
    "    processed_text = re.sub(r'\\s+', ' ', processed_text, flags=re.I)\n",
    "    processed_text = re.sub(r'^b\\s+', '', processed_text)\n",
    "\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in processed_text.split() if word not in stop_words]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    processed_text = ' '.join(tokens)\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09f5951c-55e3-4bdb-9987-627628626ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Reading from file\n",
      "[TEST] Reading from file\n"
     ]
    }
   ],
   "source": [
    "# from tqdm.notebook import tqdm\n",
    "import os\n",
    "from tqdm.auto import tqdm  # for notebooks\n",
    "tqdm.pandas()\n",
    "\n",
    "preprocessed_file_path_train = 'pre_train.csv'\n",
    "if not os.path.exists(preprocessed_file_path_train):\n",
    "    print(\"[TRAIN] Preprocessing text...\")\n",
    "    train_data['text'] = train_data['text'].progress_apply(preprocess_text)\n",
    "    print(\"[TRAIN] Preprocessing text done.\")\n",
    "    train_data.to_csv(preprocessed_file_path_train, index=False)\n",
    "else:\n",
    "    print(\"[TRAIN] Reading from file\")\n",
    "    train_data = pd.read_csv(preprocessed_file_path_train)\n",
    "\n",
    "preprocessed_file_path_test = 'pre_test.csv'\n",
    "if not os.path.exists(preprocessed_file_path_test):\n",
    "    print(\"[TEST] Preprocessing text...\")\n",
    "    test_data['text'] = test_data['text'].progress_apply(preprocess_text)\n",
    "    print(\"[TEST] Preprocessing text done.\")\n",
    "    test_data.to_csv(preprocessed_file_path_test, index=False)\n",
    "else:\n",
    "    print(\"[TEST] Reading from file\")\n",
    "    test_data = pd.read_csv(preprocessed_file_path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3c105bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.head(1000)\n",
    "test_data = test_data.head(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962b0e29-7661-418f-9758-3c335fea0689",
   "metadata": {},
   "source": [
    "## Data vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26b16adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from datasketch import MinHashLSH, MinHash\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb89909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def jaccard_similarity(a, b):\n",
    "#     intersection_size = len(set(a) & set(b))\n",
    "#     union_size = len(set(a) | set(b))\n",
    "\n",
    "#     return intersection_size / union_size if union_size > 0 else 0.0\n",
    "\n",
    "def jaccard_similarity(a, b):\n",
    "    \n",
    "    a = a.toarray()\n",
    "    b = b.toarray()\n",
    "    \n",
    "    set_a = set(a)\n",
    "    set_b = set(b)\n",
    "    \n",
    "    intersection_size = len(np.intersect1d(a, b))\n",
    "    union_size = len(set_a) + len(set_b) - intersection_size\n",
    "    \n",
    "    return intersection_size / union_size if union_size > 0 else 0.0\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    set1 = set1.toarray()\n",
    "    set2 = set2.toarray()\n",
    "    intersection_size = len(np.intersect1d(set1, set2))\n",
    "    union_size = len(np.union1d(set1, set2))\n",
    "    \n",
    "    if union_size == 0:\n",
    "        return 0.0  # Jaccard similarity is 0 if the sets are both empty\n",
    "    else:\n",
    "        return intersection_size / union_size\n",
    "\n",
    "from scipy.spatial.distance import jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a0162b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_aslist = test_data['text'].tolist()\n",
    "train_data_aslist = train_data['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7691bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "k_neighbors = 15  # Number of neighbors for K-NN\n",
    "threshold = 0.8  # Similarity threshold for LSH\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "# vectorizer = TfidfVectorizer(max_features=100)\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(train_data['text'])\n",
    "X_test_tfidf = vectorizer.transform(test_data['text'])\n",
    "\n",
    "# Build MinHash LSH index\n",
    "num_permutations = [16, 32, 64]\n",
    "\n",
    "# Convert sparse matrices to dense arrays\n",
    "X_train_dense = X_train_tfidf.toarray()\n",
    "X_test_dense = X_test_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bc2a166",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1008/697647495.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrue_knn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk_neighbors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'brute'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjaccard_similarity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrue_knn_distances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_knn_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrue_knn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meffective_metric_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             chunked_results = list(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 pairwise_distances_chunked(\n\u001b[1;32m    861\u001b[0m                     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   2015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2016\u001b[0m             \u001b[0mX_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2017\u001b[0;31m         \u001b[0mD_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpairwise_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2018\u001b[0m         if (X is Y or Y is None) and PAIRWISE_DISTANCE_FUNCTIONS.get(\n\u001b[1;32m   2019\u001b[0m             \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[1;32m   2193\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2195\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0meffective_n_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1765\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m     \u001b[0;31m# enforce a threading backend to prevent data communication overhead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36m_pairwise_callable\u001b[0;34m(X, Y, metric, force_all_finite, **kwds)\u001b[0m\n\u001b[1;32m   1807\u001b[0m         \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1809\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1008/4155667762.py\u001b[0m in \u001b[0;36mjaccard_similarity\u001b[0;34m(set1, set2)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mset2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mintersection_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersect1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0munion_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0munion_size\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36munion1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munion1d\u001b[0;34m(ar1, ar2)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m     \"\"\"\n\u001b[0;32m--> 781\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mar2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         ret = _unique1d(ar, return_index, return_inverse, return_counts, \n\u001b[0m\u001b[1;32m    275\u001b[0m                         equal_nan=equal_nan)\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "true_knn = NearestNeighbors(n_neighbors=k_neighbors, algorithm='brute', metric='jaccard').fit(X_train_dense)\n",
    "true_knn_distances, true_knn_indices = true_knn.kneighbors(X_test_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9555ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_knn_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bd62a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_knn_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e4e06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsh_knn(candidates, train_set, test_doc):\n",
    "    similarities = [(idx, jaccard_similarity(set(test_doc.split()), set(train_set[idx].split())))\n",
    "                    for idx in candidates]\n",
    "\n",
    "    sorted_similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "    k_most_similar = sorted_similarities[:k_neighbors]\n",
    "\n",
    "    return k_most_similar, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f48a67-b32c-4a86-b5dc-5b35e246ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_perm in num_permutations:\n",
    "    lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
    "    \n",
    "    minhash_signatures_train = []\n",
    "    for doc in train_data_aslist:\n",
    "        minhash = MinHash(num_perm=num_perm)\n",
    "        for word in doc:\n",
    "            minhash.update(word.encode('utf8'))\n",
    "#         print(minhash.hashvalues)\n",
    "        minhash_signatures_train.append(minhash)\n",
    "#         print(minhash_signatures_train)\n",
    "        lsh.insert(len(minhash_signatures_train) - 1, minhash)\n",
    "\n",
    "    start_query_time = time.time()\n",
    "\n",
    "    correct_predictions = 0\n",
    "    total_fraction = 0\n",
    "    lsh_indices = []\n",
    "    lsh_distances = []\n",
    "    for i, doc in enumerate(test_data_aslist):\n",
    "        minhash = MinHash(num_perm=num_perm)\n",
    "        for word in doc:\n",
    "            minhash.update(word.encode('utf8'))\n",
    "\n",
    "        candidates = lsh.query(minhash)\n",
    "#         print(candidates)\n",
    "        similarities = true_knn_distances[i]\n",
    "#         print(similarities)\n",
    "        true_indices = true_knn_indices[i]\n",
    "#         print(true_knn_indices)\n",
    "        \n",
    "        \n",
    "        num_of_true_docs = sum(1 for item in candidates if item in true_indices)\n",
    "        \n",
    "        total_fraction += (num_of_true_docs / true_indices.shape[0])\n",
    "#         print(\"For test doc: \", i, \" found \", num_of_true_docs, \"/15\")\n",
    "\n",
    "        if candidates:\n",
    "            bucket_indices, bucket_distances = lsh_knn(candidates, train_data_aslist, doc)\n",
    "            lsh_indices.append(bucket_indices)\n",
    "            lsh_distances.append(bucket_distances)\n",
    "\n",
    "        \n",
    "    end_query_time = time.time()\n",
    "    build_time = time.time()\n",
    "    query_time = end_query_time - start_query_time\n",
    "    total_time = build_time + query_time\n",
    "\n",
    "    print(f\"\\nResults for num_perm={num_perm}:\")\n",
    "    print(f\"LSH Index Creation Time (BuildTime): {build_time:.4f} seconds\")\n",
    "    print(f\"Total Query Time (QueryTime): {query_time:.4f} seconds\")\n",
    "    print(f\"Total Time (TotalTime): {total_time:.4f} seconds\")\n",
    "    accuracy = total_fraction / len(test_data_aslist)\n",
    "    print(f\"Fraction of True K-most similar documents: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
